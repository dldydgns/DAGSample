from airflow import DAG
from airflow.operators.python import PythonOperator
from datetime import datetime
import boto3, os

BUCKET_NAME = os.getenv("S3_BUCKET_NAME")
FILE_KEY = os.getenv("S3_FILE_KEY")

def write_hello_to_s3():
    s3 = boto3.client("s3")
    s3.put_object(Bucket=BUCKET_NAME, Key=FILE_KEY, Body="hello")
    print(f"✅ Created {FILE_KEY} with 'hello' in {BUCKET_NAME}")

def append_world_to_s3():
    s3 = boto3.client("s3")
    obj = s3.get_object(Bucket=BUCKET_NAME, Key=FILE_KEY)
    content = obj["Body"].read().decode("utf-8")
    updated_content = content + " world!"
    s3.put_object(Bucket=BUCKET_NAME, Key=FILE_KEY, Body=updated_content)
    print(f"✅ Appended ' world!' to {FILE_KEY}")

with DAG(
    dag_id="s3_test_file_workflow",
    start_date=datetime(2025, 11, 5),
    schedule_interval=None,
    catchup=False,
    tags=["example"],
) as dag:
    create_file = PythonOperator(
        task_id="create_test_txt",
        python_callable=write_hello_to_s3,
    )

    append_file = PythonOperator(
        task_id="append_world_to_test_txt",
        python_callable=append_world_to_s3,
    )

    create_file >> append_file
